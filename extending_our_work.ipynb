{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0b2d89-2ca1-40eb-843a-9f3eddc4944c",
   "metadata": {},
   "source": [
    "# Using and extending `genjax.vi`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaebd2e-4091-494f-83fd-3b221dacd087",
   "metadata": {},
   "source": [
    "This notebook is intended as a tutorial: a gentle guide to the usage of our system on new problems, which illustrates how several parts of the system work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd955977-8d0a-4a68-81c7-f031f17c348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import genjax\n",
    "from genjax import vi\n",
    "from extras import beta_implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a30e37f-e3a1-430a-b382-79e0529ebdc4",
   "metadata": {},
   "source": [
    "## Implementing new models and guides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7330bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "#####################\n",
    "# Model & Guide\n",
    "#####################\n",
    "\n",
    "@genjax.gen\n",
    "def model():\n",
    "    f = genjax.beta(0.5, 0.5) @ \"latent_fairness\"\n",
    "    _ = genjax.tfp_bernoulli(f) @ \"obs\"\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def guide(log_alpha, log_beta):\n",
    "    beta_implicit(jnp.exp(log_alpha), jnp.exp(log_beta)) @ \"latent_fairness\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3998e-d46c-4cec-934c-acb48589e1bd",
   "metadata": {},
   "source": [
    "With a model and guide program defined, we can construct a loss function in two ways: (a) using a standard library loss function (like `genjax.vi.elbo`) or (b) by writing our own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ef363-24b7-441e-8ed3-9f05d2323987",
   "metadata": {},
   "source": [
    "### Writing our own loss\n",
    "\n",
    "Let's write our own to get a feel for what that looks like.\n",
    "\n",
    "First, we'll define two utility interfaces which wrap functionality exposed by `GenerativeFunction` - the model type of Gen. These are just for our own convenience, to make the code a bit nicer to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a303395-f721-4fa0-a937-cda093480599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a trace.\n",
    "def sim(g: genjax.GenerativeFunction, args):\n",
    "    key = adevjax.reap_key() # gain access to a fresh PRNG key\n",
    "    tr = g.simulate(key, args)\n",
    "    return tr\n",
    "\n",
    "# Score constraints.\n",
    "def density(g, chm, args):\n",
    "    key = adevjax.reap_key() # gain access to a fresh PRNG key\n",
    "    _, score = g.assess(key, chm, args)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245fa88e-7836-41c2-b8aa-4be7b6fe75f0",
   "metadata": {},
   "source": [
    "Now, the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "702e667a-ae2c-4bb5-8a93-7211862a567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adevjax\n",
    "from genjax.typing import Tuple\n",
    "\n",
    "def elbo(\n",
    "    p: genjax.GenerativeFunction,\n",
    "    q: genjax.GenerativeFunction,\n",
    "    data: genjax.ChoiceMap,\n",
    "):\n",
    "    @adevjax.adev\n",
    "    def elbo_loss(p_args: Tuple, q_args: Tuple):\n",
    "        tr = sim(q, q_args)\n",
    "        q_score = tr.get_score()\n",
    "        observed = tr.get_choices().safe_merge(data)\n",
    "        p_score = density(p, observed, p_args)\n",
    "        return p_score - q_score\n",
    "\n",
    "    return adevjax.E(elbo_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4cf29-bd8c-48c6-8c8b-e7682f4accbb",
   "metadata": {},
   "source": [
    "Now, given some data, this object is something that we can construct an unbiased gradient estimator for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6858aa01-1fa5-42a7-8302-6085fec5c08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Data Generation\n",
    "#####################\n",
    "\n",
    "data = []\n",
    "for _ in range(1):\n",
    "    data.append(True)\n",
    "for _ in range(9):\n",
    "    data.append(False)\n",
    "\n",
    "data = jnp.array(data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23962668-0a2f-423d-8a80-e2930d270f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(-1.7184708, dtype=float32, weak_type=True),\n",
       " Array(1.4253635, dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective = elbo(model, guide, genjax.choice_map({\"obs\": data}))\n",
    "_, q_grads = objective.grad_estimate(key, ((), (1.0, 1.0)))\n",
    "q_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4bf25-31a8-4f17-bcb5-fc01014bd6d3",
   "metadata": {},
   "source": [
    "That all works, just like you'd expect it to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb2692-e45e-4797-a6fc-758d77e63d6e",
   "metadata": {},
   "source": [
    "### Using a standard library loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6079ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# SVI Setup\n",
    "#####################\n",
    "\n",
    "def svi_update(model, guide, optimizer):\n",
    "    def _inner(key, data, params):\n",
    "        data_chm = genjax.choice_map({\"obs\": data})\n",
    "        objective = vi.elbo(model, guide, data_chm) # Here's our objective\n",
    "        (loss, (_, params_grad)) = objective.value_and_grad_estimate(key, ((), params))\n",
    "        params_grad = jax.tree_util.tree_map(lambda v: v * -1.0, params_grad)\n",
    "        return params_grad, loss\n",
    "\n",
    "    @jax.jit\n",
    "    def updater(key, data, params, opt_state):\n",
    "        params_grad, loss = _inner(key, data, params)\n",
    "        updates, opt_state = optimizer.update(params_grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, loss, opt_state\n",
    "\n",
    "    return updater"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0b0a5-621f-4c23-88f3-3088d5fdefc5",
   "metadata": {},
   "source": [
    "Below, we setup our parameters, and update process, to prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a6b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the optimizer\n",
    "adam = optax.adam(5e-3)\n",
    "svi_updater = svi_update(model, guide, adam)\n",
    "\n",
    "# initialize parameters\n",
    "log_alpha = jnp.log(jnp.array(10.0))\n",
    "log_beta = jnp.log(jnp.array(10.0))\n",
    "\n",
    "params = (log_alpha, log_beta)\n",
    "opt_state = adam.init(params)\n",
    "\n",
    "# warm up JIT compiler\n",
    "key = jax.random.PRNGKey(0)\n",
    "_ = svi_updater(key, data, params, opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c834fc-7764-4855-ab33-38982c4373f9",
   "metadata": {},
   "source": [
    "We run our update process for 2000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0e2594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Gradient Steps\n",
    "#####################\n",
    "losses = []\n",
    "for step in range(2000):\n",
    "    key, sub_key = jax.random.split(key)\n",
    "    params, loss, opt_state = svi_updater(key, data, params, opt_state)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89237184-6971-487d-8e0d-706b9de513d8",
   "metadata": {},
   "source": [
    "Now, we can look at the trained parameters from our variational guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d153225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.115 +- 0.139\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Inferred parameters\n",
    "#####################\n",
    "log_alpha, log_beta = params\n",
    "alpha, beta = jnp.exp(log_alpha), jnp.exp(log_beta)\n",
    "\n",
    "# here we use some facts about the Beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha / (alpha + beta)\n",
    "# compute inferred standard deviation\n",
    "factor = beta / (alpha * (1.0 + alpha + beta))\n",
    "inferred_std = inferred_mean * jnp.sqrt(factor)\n",
    "print(\n",
    "    \"\\nBased on the data and our prior belief, the fairness \"\n",
    "    + \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea594c7-8822-479a-9fe9-0b1ce163f8e2",
   "metadata": {},
   "source": [
    "We can bundle this all up into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa5e755-98ef-4a17-ae9c-923415a84765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(key, data):\n",
    "    def svi_update(model, guide, optimizer):\n",
    "        def _inner(key, data, params):\n",
    "            data_chm = genjax.choice_map({\"obs\": data})\n",
    "            objective = vi.elbo(model, guide, data_chm) # Here's our objective\n",
    "            (loss, (_, params_grad)) = objective.value_and_grad_estimate(key, ((), params))\n",
    "            params_grad = jax.tree_util.tree_map(lambda v: v * -1.0, params_grad)\n",
    "            return params_grad, loss\n",
    "    \n",
    "        @jax.jit\n",
    "        def updater(key, data, params, opt_state):\n",
    "            params_grad, loss = _inner(key, data, params)\n",
    "            updates, opt_state = optimizer.update(params_grad, opt_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            return params, loss, opt_state\n",
    "    \n",
    "        return updater\n",
    "\n",
    "    # setup the optimizer\n",
    "    adam = optax.adam(5e-3)\n",
    "    svi_updater = svi_update(model, guide, adam)\n",
    "    \n",
    "    # initialize parameters\n",
    "    log_alpha = jnp.log(jnp.array(10.0))\n",
    "    log_beta = jnp.log(jnp.array(10.0))\n",
    "    \n",
    "    params = (log_alpha, log_beta)\n",
    "    opt_state = adam.init(params)\n",
    "    \n",
    "    # warm up JIT compiler\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    _ = svi_updater(key, data, params, opt_state)\n",
    "\n",
    "    losses = []\n",
    "    for step in range(2000):\n",
    "        key, sub_key = jax.random.split(key)\n",
    "        params, loss, opt_state = svi_updater(key, data, params, opt_state)\n",
    "        losses.append(loss)\n",
    "\n",
    "    log_alpha, log_beta = params\n",
    "    alpha, beta = jnp.exp(log_alpha), jnp.exp(log_beta)\n",
    "    \n",
    "    # here we use some facts about the Beta distribution\n",
    "    # compute the inferred mean of the coin's fairness\n",
    "    inferred_mean = alpha / (alpha + beta)\n",
    "    # compute inferred standard deviation\n",
    "    factor = beta / (alpha * (1.0 + alpha + beta))\n",
    "    inferred_std = inferred_mean * jnp.sqrt(factor)\n",
    "    print(\n",
    "        \"\\nBased on the data and our prior belief, the fairness \"\n",
    "        + \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9555b2-b02c-4f89-ac1b-a75460b6e438",
   "metadata": {},
   "source": [
    "Play around with this cell, and see if the inferences make sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a3fe4d2-86b2-422b-9de4-fb2da6b1b95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.835 +- 0.184\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(10):\n",
    "    data.append(True)\n",
    "for _ in range(0):\n",
    "    data.append(False)\n",
    "\n",
    "data = jnp.array(data)\n",
    "\n",
    "run_experiment(key, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36208e9-23cb-4908-b1a6-0e00857faad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(g: genjax.GenerativeFunction, args):\n",
    "    key = adevjax.reap_key() # gain access to a fresh PRNG key\n",
    "    tr = g.simulate(key, args)\n",
    "    return tr\n",
    "\n",
    "def density(g, chm, args):\n",
    "    key = adevjax.reap_key() # gain access to a fresh PRNG key\n",
    "    _, score = g.assess(key, chm, args)\n",
    "    return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
