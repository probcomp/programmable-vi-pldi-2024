{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0b2d89-2ca1-40eb-843a-9f3eddc4944c",
   "metadata": {},
   "source": [
    "# Using and extending `genjax.vi`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaebd2e-4091-494f-83fd-3b221dacd087",
   "metadata": {},
   "source": [
    "This notebook shows how to use our library to solve a new  inference task, beyond those considered in our experiments. It is intended to illustrate the usage of the library, but assumes some knowledge of variational inference. \n",
    "\n",
    "We use a very simple toy example: inferring the likely bias of a coin from multiple observed coin flips. (This same example is used in the tutorials for Pyro, another PPL that we compare to in our experiments.) The inference problem comes from [Pyro's SVI Part I tutorial](https://pyro.ai/examples/svi_part_i.html#A-simple-example).\n",
    "\n",
    "Variational inference is overkill for solving this problemâ€”in fact, the exact Bayesian posterior is analytically tractable. But it will serve to demonstrate the basic features of the library. The more complex examples in our experiments follow the same basic structure, and so understanding the code in this notebook should make it possible to understand (and modify) those more complex examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd955977-8d0a-4a68-81c7-f031f17c348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import genjax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from genjax import vi\n",
    "from extras import beta_implicit, sim, density\n",
    "\n",
    "key = jax.random.PRNGKey(314159)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a30e37f-e3a1-430a-b382-79e0529ebdc4",
   "metadata": {},
   "source": [
    "## Implementing new models and guides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc73aa-dbec-4c10-adc7-5f1e60125c5e",
   "metadata": {},
   "source": [
    "We begin by defining a model, a joint distribution over the unknown bias of the coin, and the observed sequence of flips.\n",
    "\n",
    "Models and variational families (guides) in our system are probabilistic programs. We write these using a modeling language which can be accessed via the `genjax.gen` decorator. \n",
    "\n",
    "In the code, random choices can be made using the syntax `dist(args) @ \"choice_name\"`, where `\"choice_name\"` is a unique name for the random variable being sampled. In the code below, our model defines a distribution over two random variables, and the variational family, or guide, defines a distribution over only one random variable.\n",
    "\n",
    "Although we don't show it here, deterministic (JAX traceable) code can be freely interwoven between random variable statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7330bcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BuiltinGenerativeFunction(source=<function model at 0x15f587490>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@genjax.gen\n",
    "def model():\n",
    "    f = genjax.tfp_beta(10.0, 10.0) @ \"latent_fairness\"\n",
    "    _ = genjax.tfp_flip(f) @ \"obs\"\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f3d28-6521-4ba6-be89-e639c0ef2232",
   "metadata": {},
   "source": [
    "We then define a variational family, a parametric representation of the posterior distribution. Our goal will be to infer which parameters alpha and beta make this variational family as close as possible to the true posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1114808-0cc2-49e5-a038-a34392959c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BuiltinGenerativeFunction(source=<function guide at 0x15f587e20>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@genjax.gen\n",
    "def guide(alpha, beta):\n",
    "    beta_implicit(alpha, beta) @ \"latent_fairness\"\n",
    "\n",
    "guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a573fa-de32-42e7-9c4a-5ba02ef057f3",
   "metadata": {},
   "source": [
    "To find the optimal parameters, we need to define a loss function that measures how well the guide matches the posterior, for any given alpha and beta. The standard choice of loss in variational inference is the ELBO, or evidence lower bound. \n",
    "\n",
    "Now, we can construct this loss function in two ways: (a) using a standard library loss function (like `genjax.vi.elbo`) or (b) by writing our own version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584ef363-24b7-441e-8ed3-9f05d2323987",
   "metadata": {},
   "source": [
    "### Writing our own loss\n",
    "\n",
    "Let's write our own version to get a feel for what that looks like in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "702e667a-ae2c-4bb5-8a93-7211862a567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import adevjax\n",
    "from genjax.typing import Tuple\n",
    "\n",
    "def elbo(\n",
    "    p: genjax.GenerativeFunction,\n",
    "    q: genjax.GenerativeFunction,\n",
    "    data: genjax.ChoiceMap,\n",
    "):\n",
    "\n",
    "    @adevjax.adev\n",
    "    def elbo_loss(p_args: Tuple, q_args: Tuple):\n",
    "        x, log_q = sim(q, q_args)\n",
    "        x_y = x.safe_merge(data)\n",
    "        log_p = density(p, x_y, p_args)\n",
    "        return log_p - log_q\n",
    "\n",
    "    return adevjax.E(elbo_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9af35-7004-4c2c-84bc-e8e06bbe0f28",
   "metadata": {},
   "source": [
    "A loss in GenJAX will generally be the expected value (`adevjax.E`) of a probabilistic process -- in this case, the process of simulating from Q and computing log P/Q.\n",
    "\n",
    "To write the loss, we use the language of [ADEV, a new type of AD algorithm](https://arxiv.org/pdf/2212.06386.pdf). ADEV handles automating the construction of unbiased gradient estimators for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4cf29-bd8c-48c6-8c8b-e7682f4accbb",
   "metadata": {},
   "source": [
    "Now, given a `p`, a `q`, and some `data`, this function will return an `Expectation`, an expected value objective function which we wish to acquire unbiased gradient estimators for. \n",
    "\n",
    "Let's construct some `data` to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6858aa01-1fa5-42a7-8302-6085fec5c08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True False False False False]\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Data Generation\n",
    "#####################\n",
    "\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(True)\n",
    "for _ in range(4):\n",
    "    data.append(False)\n",
    "\n",
    "data = jnp.array(data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5aace-bae9-46d2-af70-08896e29ac7c",
   "metadata": {},
   "source": [
    "Now, we can build our objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23962668-0a2f-423d-8a80-e2930d270f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Expectation(prog=ADEVProgram(source=<function elbo.<locals>.elbo_loss at 0x15f5cca60>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective = elbo(model, guide, genjax.choice_map({\"obs\": data}))\n",
    "objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3b2b9-ba8d-43be-aaa9-dc6864ee2fc6",
   "metadata": {},
   "source": [
    "And we can even construct and sample from a gradient estimator for the objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef685d1-e026-4801-961a-033b4c70b53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(-13.78883, dtype=float32, weak_type=True),\n",
       " Array(25.818943, dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, sub_key = jax.random.split(key)\n",
    "_, q_grads = objective.grad_estimate(sub_key, ((), (1.0, 1.0)))\n",
    "q_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4bf25-31a8-4f17-bcb5-fc01014bd6d3",
   "metadata": {},
   "source": [
    "The `objective.grad_estimate` method takes arguments `(key: PRNGKey, loss_args: Tuple)` and returns an unbiased estimate of the gradient of our objective. \n",
    "\n",
    "We can use these gradient estimates for stochastic optimization of the guide's parameters (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb2692-e45e-4797-a6fc-758d77e63d6e",
   "metadata": {},
   "source": [
    "### Using a standard library loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f9487-9530-4b1c-b952-e490b7d452eb",
   "metadata": {},
   "source": [
    "Of course, we can also use the standard library version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94feba7c-e704-4683-8da8-c7a8e1c37b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Expectation(prog=ADEVProgram(source=<function elbo.<locals>.elbo_loss at 0x15ffbf520>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective = genjax.vi.elbo(model, guide, genjax.choice_map({\"obs\": data}))\n",
    "objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a330520e-93a5-49ef-b24f-c41e7b2f2dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(7.713711, dtype=float32, weak_type=True),\n",
       " Array(-7.2344327, dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, sub_key = jax.random.split(key)\n",
    "_, q_grads = objective.grad_estimate(sub_key, ((), (1.0, 1.0)))\n",
    "q_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a7ea9-c0b1-4459-8846-317aabd82691",
   "metadata": {},
   "source": [
    "Now, we'll use the loss as part of a training loop, where we compute gradient estimates and perform stochastic gradient ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6079ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Parameter updater\n",
    "#####################\n",
    "\n",
    "def svi_update(model, guide, optimizer):\n",
    "    def _inner(key, data, params):\n",
    "        data_chm = genjax.choice_map({\"obs\": data})\n",
    "        objective = vi.elbo(model, guide, data_chm) # Here's our objective\n",
    "        (loss, (_, params_grad)) = objective.value_and_grad_estimate(key, ((), params))\n",
    "        params_grad = jax.tree_util.tree_map(lambda v: v * -1.0, params_grad)\n",
    "        return params_grad, loss\n",
    "\n",
    "    @jax.jit\n",
    "    def updater(key, data, params, opt_state):\n",
    "        params_grad, loss = _inner(key, data, params)\n",
    "        updates, opt_state = optimizer.update(params_grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, loss, opt_state\n",
    "\n",
    "    return updater"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0b0a5-621f-4c23-88f3-3088d5fdefc5",
   "metadata": {},
   "source": [
    "Below, we setup our parameters, and the update process, to prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a6b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the optimizer\n",
    "adam = optax.adam(5e-4)\n",
    "svi_updater = svi_update(model, guide, adam)\n",
    "\n",
    "# initialize parameters\n",
    "alpha = jnp.array(2.0)\n",
    "beta = jnp.array(2.0)\n",
    "params = (alpha, beta)\n",
    "opt_state = adam.init(params)\n",
    "\n",
    "# warm up JIT compiler\n",
    "key = jax.random.PRNGKey(2)\n",
    "_ = svi_updater(key, data, params, opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c834fc-7764-4855-ab33-38982c4373f9",
   "metadata": {},
   "source": [
    "We run our update process for 2000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e2594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Run gradient steps\n",
    "#####################\n",
    "\n",
    "for step in range(5000):\n",
    "    key, sub_key = jax.random.split(key)\n",
    "    params, loss, opt_state = svi_updater(key, data, params, opt_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89237184-6971-487d-8e0d-706b9de513d8",
   "metadata": {},
   "source": [
    "Now, we can look at the trained parameters from our variational guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d153225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.525 +- 0.207\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Inferred parameters\n",
    "#####################\n",
    "\n",
    "alpha, beta = params\n",
    "\n",
    "# here we use some facts about the Beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha / (alpha + beta)\n",
    "# compute inferred standard deviation\n",
    "factor = beta / (alpha * (1.0 + alpha + beta))\n",
    "inferred_std = inferred_mean * jnp.sqrt(factor)\n",
    "print(\n",
    "    \"\\nBased on the data and our prior belief, the fairness \"\n",
    "    + \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea594c7-8822-479a-9fe9-0b1ce163f8e2",
   "metadata": {},
   "source": [
    "We can bundle this all up into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fa5e755-98ef-4a17-ae9c-923415a84765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(key, data, loss_fn):\n",
    "    def svi_update(model, guide, optimizer):\n",
    "        def _inner(key, data, params):\n",
    "            data_chm = genjax.choice_map({\"obs\": data})\n",
    "            objective = loss_fn(model, guide, data_chm) # Here's our objective\n",
    "            (loss, (_, params_grad)) = objective.value_and_grad_estimate(key, ((), params))\n",
    "            params_grad = jax.tree_util.tree_map(lambda v: v * -1.0, params_grad)\n",
    "            return params_grad, loss\n",
    "    \n",
    "        @jax.jit\n",
    "        def updater(key, data, params, opt_state):\n",
    "            params_grad, loss = _inner(key, data, params)\n",
    "            updates, opt_state = optimizer.update(params_grad, opt_state, params)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            return params, loss, opt_state\n",
    "    \n",
    "        return updater\n",
    "\n",
    "    # setup the optimizer\n",
    "    adam = optax.adam(5e-4)\n",
    "    svi_updater = svi_update(model, guide, adam)\n",
    "    \n",
    "    # initialize parameters\n",
    "    alpha = jnp.array(15.0)\n",
    "    beta = jnp.array(15.0)\n",
    "    \n",
    "    params = (alpha, beta)\n",
    "    opt_state = adam.init(params)\n",
    "    \n",
    "    # warm up JIT compiler\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    _ = svi_updater(key, data, params, opt_state)\n",
    "\n",
    "    losses = []\n",
    "    for step in range(5000):\n",
    "        key, sub_key = jax.random.split(key)\n",
    "        params, loss, opt_state = svi_updater(key, data, params, opt_state)\n",
    "        losses.append(loss)\n",
    "\n",
    "    alpha, beta = params\n",
    "    \n",
    "    # here we use some facts about the Beta distribution\n",
    "    # compute the inferred mean of the coin's fairness\n",
    "    inferred_mean = alpha / (alpha + beta)\n",
    "    # compute inferred standard deviation\n",
    "    factor = beta / (alpha * (1.0 + alpha + beta))\n",
    "    inferred_std = inferred_mean * jnp.sqrt(factor)\n",
    "    print(\n",
    "        \"\\nBased on the data and our prior belief, the fairness \"\n",
    "        + \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9555b2-b02c-4f89-ac1b-a75460b6e438",
   "metadata": {},
   "source": [
    "Play around with this cell, and see if the inferences make sense! Note that the model beta prior `Beta(10.0, 10.0)` is symmetric, and peaked around `p = 0.5`. \n",
    "\n",
    "The prior is going to prevent the variational family from having inferred means far away from `p = 0.5`, while the data will try and pull the family away (depending on the number of `True` of `False` values observed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3fe4d2-86b2-422b-9de4-fb2da6b1b95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.549 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(8):\n",
    "    data.append(True)\n",
    "for _ in range(2):\n",
    "    data.append(False)\n",
    "\n",
    "data = jnp.array(data)\n",
    "\n",
    "run_experiment(key, data, genjax.vi.elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d292f41-f908-4b51-a9ec-9411871ebcc2",
   "metadata": {},
   "source": [
    "## Using an alternative objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782d10e9-8953-4c55-9fbe-b547e5a58db4",
   "metadata": {},
   "source": [
    "Although the ELBO is a standard choice for variational inference, other objectives are possible, and our library makes it possible to define your own. This model is simple enough that many variational objectives will yield comparable inference results, but as an example, here is [the 2-particle IWELBO objective](https://arxiv.org/abs/1509.00519) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d56ad497-64b4-454f-b49f-86a7c5afe3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.tree_util as jtu\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "def iwelbo(\n",
    "    p: genjax.GenerativeFunction,\n",
    "    q: genjax.GenerativeFunction,\n",
    "    data: genjax.ChoiceMap,\n",
    "):\n",
    "    @adevjax.adev\n",
    "    def loss(p_args: Tuple, q_args: Tuple):\n",
    "        s1 = sim(q, q_args)\n",
    "        s2 = sim(q, q_args)\n",
    "        latents, q_scores = jtu.tree_map(lambda v1, v2: jnp.hstack([v1, v2]), s1, s2)\n",
    "        \n",
    "        def score_against_model(proposal):\n",
    "            observed = proposal.safe_merge(data)\n",
    "            return density(p, observed, p_args)\n",
    "            \n",
    "        p_scores = jax.vmap(score_against_model)(latents)\n",
    "        return logsumexp(p_scores - q_scores) - jnp.log(2)\n",
    "\n",
    "    return adevjax.E(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c90a1-9001-4811-b9aa-88d7007d0686",
   "metadata": {},
   "source": [
    "Note that after redefining the objective, we can retrain, without other changes to the code. This is in contrast to systems like Pyro, where defining new objective functions can require involved changes to library internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69ca6d58-defb-4fc3-85ae-ca3302015a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.543 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "run_experiment(key, data, iwelbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6d1b9-1bf1-432c-a2b8-108d527f58f8",
   "metadata": {},
   "source": [
    "Pretty convenient! In this case, the change to the loss doesn't affect the result significantly, but it's better to have the option to explore different objective functions than not!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908758d-cf21-4828-b871-23588875613b",
   "metadata": {},
   "source": [
    "## Implementing new distributions with gradient strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec28e3-0fcf-449c-b991-09ee417db88c",
   "metadata": {},
   "source": [
    "At the top of this file, we had to import the Beta distribution (as `beta_implicit`) to use in our guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ffec31d-6e9f-4cf1-ba2c-30c3c7275265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extras import beta_implicit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b87acf-2c59-468b-9490-4973f51fbcbf",
   "metadata": {},
   "source": [
    "Each primitive distribution in our library comes equipped with some default strategy for estimating gradients of expected values under the distribution in question. \n",
    "\n",
    "New primitives, with new gradient estimation strategies, can be added modularly; see the code in `./extras/beta_implicit.py` for an example of how this is done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
