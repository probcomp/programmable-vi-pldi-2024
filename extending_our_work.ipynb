{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0b2d89-2ca1-40eb-843a-9f3eddc4944c",
   "metadata": {},
   "source": [
    "# Extending our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd955977-8d0a-4a68-81c7-f031f17c348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "from genjax import vi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d7182-2fe3-4ecd-aecc-f846c3792a88",
   "metadata": {},
   "source": [
    "This notebook illustrates a number of ways to build upon our work and implementation. Here are a few:\n",
    "\n",
    "* (**Extending ADEV, the automatic differentiation algorithm, with new samplers equipped with gradient strategies.**) After implementing the ADEV interfaces for these objects, they can be freely lifted into the `Distribution` type of our language, and can be used in modeling and guide code. We illustrate this process by implementing `beta_implicit`, and using it in a model and guide program from the Pyro tutorials.\n",
    "* (**Using a standard loss function (like `genjax.vi.elbo`) with new models and guides.**) By virtue of the programmability of our system, this is a standard means of extending our work. This extension is covered in the tutorial for the first case, above.\n",
    "* (**Implementing new loss functions, by utilizing the modeling interfaces in our language.**) We illustrate this process by implementing [SDOS](https://arxiv.org/abs/2103.01030), an estimator for a symmetric KL divergence, using our language and automated the derivation of gradients for a guide program.\n",
    "\n",
    "We cover each of these possible extensions in turn below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef6e48-df95-4ec0-b8d0-dd5d8548a8bc",
   "metadata": {},
   "source": [
    "## Implementing new samplers for ADEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79f2a6-8129-476c-ad22-a14b1465b777",
   "metadata": {},
   "source": [
    "ADEV is an extensible AD algorithm: users can implement new samplers equipped with gradient strategies, and use them in ADEV programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e694207b-e4dd-4ac3-a1b0-e0a99f928f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adevjax import ADEVPrimitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa36ce-716e-49aa-82a7-0272baef2a8c",
   "metadata": {},
   "source": [
    "### Implementing a `beta_implicit` sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0638fdd-2ae2-4e72-a8e9-28a0ecd073be",
   "metadata": {},
   "source": [
    "In [ADEV appendix B.7](https://arxiv.org/pdf/2212.06386.pdf), the author's outline a gradient strategy for distribution samplers when the CDF is available. In the literature, this is called implicit differentiation.\n",
    "\n",
    "Several libraries take advantage of this strategy already, including the `distributions` module of [TensorFlow Probability](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions), for distributions like [Beta](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Beta).\n",
    "\n",
    "Our system enables extenders to take advantage of these strategies directly (when 3rd party libraries _already implement differentiation rules_ via JAX's native JVP rule system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5416553-274f-4068-82b4-484935e88ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import jax\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "# Defining a new primitive.\n",
    "@dataclass\n",
    "class BetaIMPLICIT(ADEVPrimitive):\n",
    "    # `flatten` is a method which is required to register this type as\n",
    "    # a JAX PyTree type.\n",
    "    def flatten(self):\n",
    "        return (), ()\n",
    "\n",
    "    # New primitives require a `sample` implementation, whose signature is:\n",
    "    # sample(self, key: PRNGKey, *args)\n",
    "    # where `PRNGKey` is the type of JAX PRNG keys.\n",
    "    def sample(self, key, alpha, beta):\n",
    "        v = tfd.Beta(concentration1=alpha, concentration0=beta).sample(seed=key)\n",
    "        return v\n",
    "\n",
    "    # New primitives require an implementation for their gradient strategy\n",
    "    # in the `jvp_estimate` method.\n",
    "    #\n",
    "    # This method is called by the ADEV interpreter, and gets access\n",
    "    # to primals, tangents, and two continuations for the rest of the computation.\n",
    "    def jvp_estimate(self, key, primals, tangents, konts):\n",
    "        kpure, kdual = konts\n",
    "\n",
    "        # Because TFP already overloads their Beta sampler with implicit\n",
    "        # differentiation rules for JVP, we directly utilize their rules.\n",
    "        def _inner(alpha, beta):\n",
    "            # Invoking TFP's Implicit reparametrization:\n",
    "            # https://github.com/tensorflow/probability/blob/v0.23.0/tensorflow_probability/python/distributions/beta.py#L292-L306\n",
    "            x = tfd.Beta(concentration1=alpha, concentration0=beta).sample(seed=key)\n",
    "            return x\n",
    "\n",
    "        # We invoke JAX's JVP (which utilizes TFP's registered implicit differentiation\n",
    "        # rule for Beta) to get a primal and tanget out.\n",
    "        primal_out, tangent_out = jax.jvp(_inner, primals, tangents)\n",
    "\n",
    "        # Then, we give the result to the ADEV dual'd continuation, to continue\n",
    "        # ADEV's forward mode.\n",
    "        return kdual((primal_out,), (tangent_out,))\n",
    "\n",
    "\n",
    "# Creating an instance, to be exported and used as a sampler.\n",
    "adev_beta_implicit = BetaIMPLICIT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97424e0d-3186-4884-a7df-bd982593cfa9",
   "metadata": {},
   "source": [
    "Now, with a new ADEV sampler in hand, we lift it to a `genjax.vi.ADEVDistribution` - a type of distribution which provides compatibility with Gen's generative computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2387f631-b3dc-4b2e-a474-9a09aea70bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_implicit = vi.ADEVDistribution.new(\n",
    "    adev_beta_implicit, lambda v, alpha, beta: tfd.Beta(alpha, beta).log_prob(v)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ca902-5e2e-4a0d-9c57-b84305d67e36",
   "metadata": {},
   "source": [
    "This object can now be used in guide code, as part of variational inference learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a30e37f-e3a1-430a-b382-79e0529ebdc4",
   "metadata": {},
   "source": [
    "## Implementing new models and guides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace649b-66c0-4818-9816-fda2c7969a94",
   "metadata": {},
   "source": [
    "In this section, we'll illustrate how to use our system with new model and guide programs. We'll directly use our `beta_implicit` from above to implement a tutorial from Pyro's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7330bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "\n",
    "#####################\n",
    "# Model & Guide\n",
    "#####################\n",
    "@genjax.gen\n",
    "def model():\n",
    "    f = genjax.beta(1.0, 1.0) @ \"latent_fairness\"\n",
    "    _ = genjax.tfp_bernoulli(f) @ \"obs\"\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def guide(log_alpha, log_beta):\n",
    "    beta_implicit(jnp.exp(log_alpha), jnp.exp(log_beta)) @ \"latent_fairness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6079ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# SVI Setup\n",
    "#####################\n",
    "def svi_update(model, guide, optimizer):\n",
    "    def _inner(key, data, params):\n",
    "        data_chm = genjax.choice_map({\"obs\": data})\n",
    "        objective = vi.elbo(model, guide, data_chm)\n",
    "        (loss, (_, params_grad)) = objective.value_and_grad_estimate(key, ((), params))\n",
    "        params_grad = jax.tree_util.tree_map(lambda v: v * -1.0, params_grad)\n",
    "        return params_grad, loss\n",
    "\n",
    "    @jax.jit\n",
    "    def updater(key, data, params, opt_state):\n",
    "        params_grad, loss = _inner(key, data, params)\n",
    "        updates, opt_state = optimizer.update(params_grad, opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, loss, opt_state\n",
    "\n",
    "    return updater\n",
    "\n",
    "\n",
    "# setup the optimizer\n",
    "adam = optax.adam(5e-3)\n",
    "svi_updater = svi_update(model, guide, adam)\n",
    "\n",
    "# initialize parameters\n",
    "log_alpha = jnp.log(jnp.array(10.0))\n",
    "log_beta = jnp.log(jnp.array(10.0))\n",
    "\n",
    "params = (log_alpha, log_beta)\n",
    "opt_state = adam.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f066d792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Data Generation\n",
    "#####################\n",
    "data = []\n",
    "for _ in range(1):\n",
    "    data.append(True)\n",
    "for _ in range(9):\n",
    "    data.append(False)\n",
    "\n",
    "data = jnp.array(data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "56a6b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm up JIT compiler\n",
    "key = jax.random.PRNGKey(0)\n",
    "_ = svi_updater(key, data, params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0e2594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Gradient Steps\n",
    "#####################\n",
    "losses = []\n",
    "for step in range(2000):\n",
    "    key, sub_key = jax.random.split(key)\n",
    "    params, loss, opt_state = svi_updater(key, data, params, opt_state)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d153225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the data and our prior belief, the fairness of the coin is 0.184 +- 0.160\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "# Inferred parameters\n",
    "#####################\n",
    "log_alpha, log_beta = params\n",
    "alpha, beta = jnp.exp(log_alpha), jnp.exp(log_beta)\n",
    "\n",
    "# here we use some facts about the Beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha / (alpha + beta)\n",
    "# compute inferred standard deviation\n",
    "factor = beta / (alpha * (1.0 + alpha + beta))\n",
    "inferred_std = inferred_mean * jnp.sqrt(factor)\n",
    "print(\n",
    "    \"\\nBased on the data and our prior belief, the fairness \"\n",
    "    + \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c8729d-044e-40a0-9511-8da7a028b98b",
   "metadata": {},
   "source": [
    "## Implementing new loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8d9fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
