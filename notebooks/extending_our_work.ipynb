{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0b2d89-2ca1-40eb-843a-9f3eddc4944c",
   "metadata": {},
   "source": [
    "# Extending our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd955977-8d0a-4a68-81c7-f031f17c348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genjax import vi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3d7182-2fe3-4ecd-aecc-f846c3792a88",
   "metadata": {},
   "source": [
    "This notebook illustrates a number of ways to build upon our work and implementation. Here are a few:\n",
    "\n",
    "* (**Extending ADEV, the automatic differentiation algorithm, with new samplers equipped with gradient strategies.**) After implementing the ADEV interfaces for these objects, they can be freely lifted into the `Distribution` type of our language, and can be used in modeling and guide code. We illustrate this process by implementing `beta_implicit`, and using it in a model and guide program from the Pyro tutorials.\n",
    "* (**Using a standard loss function (like `genjax.vi.elbo`) with new models and guides.**) By virtue of the programmability of our system, this is a standard means of extending our work. This extension is covered in the tutorial for the first case, above.\n",
    "* (**Implementing new loss functions, by utilizing the modeling interfaces in our language.**) We illustrate this process by implementing [SDOS](https://arxiv.org/abs/2103.01030), an estimator for a symmetric KL divergence, using our language and automated the derivation of gradients for a guide program.\n",
    "\n",
    "We cover each of these possible extensions in turn below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef6e48-df95-4ec0-b8d0-dd5d8548a8bc",
   "metadata": {},
   "source": [
    "## Implementing new samplers for ADEV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79f2a6-8129-476c-ad22-a14b1465b777",
   "metadata": {},
   "source": [
    "ADEV is an extensible AD algorithm: users can implement new samplers equipped with gradient strategies, and use them in ADEV programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e694207b-e4dd-4ac3-a1b0-e0a99f928f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adevjax import ADEVPrimitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa36ce-716e-49aa-82a7-0272baef2a8c",
   "metadata": {},
   "source": [
    "### Implementing a `beta_implicit` sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0638fdd-2ae2-4e72-a8e9-28a0ecd073be",
   "metadata": {},
   "source": [
    "In [ADEV appendix B.7](https://arxiv.org/pdf/2212.06386.pdf), the author's outline a gradient strategy for distribution samplers when the CDF is available. In the literature, this is called implicit differentiation.\n",
    "\n",
    "Several libraries take advantage of this strategy already, including the `distributions` module of [TensorFlow Probability](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions), for distributions like [Beta](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/Beta).\n",
    "\n",
    "Our system enables extenders to take advantage of these strategies directly (when 3rd party libraries _already implement differentiation rules_ via JAX's native JVP rule system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5416553-274f-4068-82b4-484935e88ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a new primitive.\n",
    "@dataclass\n",
    "class BetaIMPLICIT(ADEVPrimitive):\n",
    "    # `flatten` is a method which is required to register this type as\n",
    "    # a JAX PyTree type.\n",
    "    def flatten(self):\n",
    "        return (), ()\n",
    "\n",
    "    # New primitives require a `sample` implementation, whose signature is:\n",
    "    # sample(self, key: PRNGKey, *args)\n",
    "    # where `PRNGKey` is the type of JAX PRNG keys.\n",
    "    def sample(self, key, alpha, beta):\n",
    "        v = tfd.Beta(concentration1=alpha, concentration0=beta).sample(seed=key)\n",
    "        return v\n",
    "\n",
    "    # New primitives require an implementation for their gradient strategy\n",
    "    # in the `jvp_estimate` method.\n",
    "    #\n",
    "    # This method is called by the ADEV interpreter, and gets access\n",
    "    # to primals, tangents, and two continuations for the rest of the computation.\n",
    "    def jvp_estimate(self, key, primals, tangents, konts):\n",
    "        kpure, kdual = konts\n",
    "\n",
    "        # Because TFP already overloads their Beta sampler with implicit\n",
    "        # differentiation rules for JVP, we directly utilize their rules.\n",
    "        def _inner(alpha, beta):\n",
    "            # Invoking TFP's Implicit reparametrization:\n",
    "            # https://github.com/tensorflow/probability/blob/v0.23.0/tensorflow_probability/python/distributions/beta.py#L292-L306\n",
    "            x = tfd.Beta(concentration1=alpha, concentration0=beta).sample(seed=key)\n",
    "            return x\n",
    "\n",
    "        # We invoke JAX's JVP (which utilizes TFP's registered implicit differentiation\n",
    "        # rule for Beta) to get a primal and tanget out.\n",
    "        primal_out, tangent_out = jax.jvp(_inner, primals, tangents)\n",
    "\n",
    "        # Then, we give the result to the ADEV dual'd continuation, to continue\n",
    "        # ADEV's forward mode.\n",
    "        return kdual((primal_out,), (tangent_out,))\n",
    "\n",
    "# Creating an instance, to be exported and used as a sampler.\n",
    "beta_implicit = BetaIMPLICIT()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97424e0d-3186-4884-a7df-bd982593cfa9",
   "metadata": {},
   "source": [
    "Now, with a new ADEV sampler in hand, we lift it to a `genjax.vi.ADEVDistribution` - a type of distribution which provides compatibility with Gen's generative computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2387f631-b3dc-4b2e-a474-9a09aea70bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_implicit = vi.ADEVDistribution(\n",
    "    beta_implicit, \n",
    "    lambda v: v\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ca902-5e2e-4a0d-9c57-b84305d67e36",
   "metadata": {},
   "source": [
    "This object can now be used in guide code, as part of variational inference learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a30e37f-e3a1-430a-b382-79e0529ebdc4",
   "metadata": {},
   "source": [
    "## Implementing new models and guides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace649b-66c0-4818-9816-fda2c7969a94",
   "metadata": {},
   "source": [
    "In this section, we'll illustrate how to use our system with new model and guide programs. We'll directly use our `beta_implicit` from above to implement a tutorial from Pyro's documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c8729d-044e-40a0-9511-8da7a028b98b",
   "metadata": {},
   "source": [
    "## Implementing new loss functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
